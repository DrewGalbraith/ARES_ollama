<h2 align="center">ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</h2>

<p align="center">

  <a>
  <img alt="Static Badge" src="https://img.shields.io/badge/release-v0.1.0-blue?style=flat&link=https%3A%2F%2Fpython.org%2F">
  </a>

  <a>
  <img alt="Static Badge" src="https://img.shields.io/badge/Read-ARES%20Paper-blue?style=flat&link=https%3A%2F%2Farxiv.org%2Fabs%2F2311.09476">
  </a>

  <a>
    <img alt="Static Badge" src="https://img.shields.io/badge/read-documentation-purple?style=flat&link=https%3A%2F%2Fares-ai.vercel.app%2F">
  </a>

  <a href="https://colab.research.google.com/drive/1lc8Tkcair7wWZVbsdNKmfSM5rbAqOeeO#scrollTo=03609iqyArxM" target="_blank">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
  </a>

  <a>
  <img alt="Static Badge" src="https://img.shields.io/badge/Made%20with-Python-red?style=flat&link=https%3A%2F%2Fpython.org%2F">
  </a>

</p>

---

### Implementing ARES

To utilize ARES for evaluating your Retrieval-Augmented Generation (RAG) system, ensure you have:

- A **Human Preference Validation Set** ‚Äî Annotated query-document-answer triples assessing evaluation criteria (context relevance, answer faithfulness, and/or answer relevance), with a minimum of 50 examples (ideally several hundred).
- A **Few-Shot Example Set** ‚Äî Selected examples for scoring the context relevance, answer faithfulness, and/or answer relevance within your system.
- A **Large Unlabeled Data Set** ‚Äî A comprehensive collection of query-document-answer triples generated by your RAG system for scoring.

### ARES Training Pipeline

Follow these steps to prepare your RAG system for ARES scoring:

1. **Synthetic Data Generation** ‚Äî Create synthetic queries and answers from in-domain passages.
2. **LLM Judge Preparation** ‚Äî Fine-tune Large Language Model (LLM) judges on the synthetically-generated data to prepare for RAG system scoring.
3. **Evaluation Deployment** ‚Äî Assess your RAG system with the trained LLM judges, focusing on key performance metrics.


Note: We also allow users to skip Steps #1 and #2 deploying a zero/few-shot LLM-as-a-Judge
‚Äã
### ‚öôÔ∏è Installation
‚Äã
To install the necessary dependencies, run the following commands:
‚Äã
````
pip install ares-ai
````
‚Äã
Optional: Initalize OpenAI or TogetherAI API key with the following command:
````
export OPENAI_API_KEY=<your key here>
export TOGETHER_API_KEY=<your key here>
````

### üöÄ Quick Start

To get started with ARES, you'll need to set up your configuration. Below is is an example of how to structure your configuration for ARES.

```python

from ares import ARES

synth_config = { 
    "document_filepaths": [<document_filepath>], 
    "few_shot_prompt_filename": <few_shot_filepath>, 
    "synthetic_queries_filenames": [<synthetic_queries_filepath>],
    "model_choice": <model_choice>, # Default model is "microsoft/deberta-v3-large"
    "documents_sampled": 10000 
}

classifier_config = {
    "classification_dataset": [<classification_dataset_filepath>],
    "test_set_selection": <test_set_selection_filepath>, 
    "label_column": [<labels>], 
    "model_choice": "microsoft/deberta-v3-large", # Default model is "microsoft/deberta-v3-large"
    "num_epochs": 10, 
    "patience_value": 3, 
    "learning_rate": 5e-6
}

ppi_config = { 
    "evaluation_datasets": [<eval_dataset_filepath>],
    "few_shot_examples_filepath": <few_shot_filepath>,
    "checkpoints": [<checkpoint_filepath>],
    "labels": [<labels>], 
    "model_choice": <model_choice>, # Default model is "microsoft/deberta-v3-large"
    "GPT_scoring": <True or False>, 
    "gold_label_path": <gold_label_filepath>, 
    "swap_human_labels_for_gpt4_labels": False
}

ares_module = ARES(synthetic_query_generator=synth_config, 
classifier_model=classifier_config, ppi=ppi_config)
results = ares_module.run()
print(results)

```
‚Äã
For `evaluation_datasets`, we expect a list of filepaths to query-passage-answer TSVs for each RAG configuration you wish to score.

If you want to use few-shot GPT scoring, switch `GPT_scoring` to `True`. You can leave the `checkpoints` list as blank and specify the GPT model with the tag `--gpt_model <model selected>`.
‚Äã

Note: For examples files of `evaluation_datasets` and `gold_label_path`, please see `example_files/evaluation_datasets.tsv` for formatting.

## Results Replication

We include synthetic datasets for key experimental results in `synthetic_datasets`. The few-shot prompts used for generation and evaluation are included in `datasets`. We also include instructions for fine-tuning LLM judges in the paper itself. Please reach out to jonsaadfalcon@stanford.edu or manihani@stanford.edu if you have any further questions.

## Citation

To cite our work, please use the following Bibtex:

````
@misc{saadfalcon2023ares,
      title={ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems}, 
      author={Jon Saad-Falcon and Omar Khattab and Christopher Potts and Matei Zaharia},
      year={2023},
      eprint={2311.09476},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
````

# Appendix
### Machine requirements and setup when not using OpenAI API
**Machine requirements**

- Over ~100 GB of available disk space
- GPU
    - Should work: A100 (e.g. `Standard_NC24ads_A100_v4` on Azure)
    - Does not work:
        - Tested on 2023-12-17 with both `Standard_NC6s_v3` and `Standard_NC12s_v3`, and ran into this error: `torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 15.77 GiB total capacity; 15.12 GiB already allocated; 95.44 MiB free; 15.12 GiB reserved in total by PyTorch)`


**Machine setup**

For example, on an Azure VM running Linux (ubuntu 20.04), you will need to do the following:
- Install conda
    - First set of commands (can copy-paste multiple lines)
        - `wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`
        - `chmod +x Miniconda3-latest-Linux-x86_64.sh`
        - `./Miniconda3-latest-Linux-x86_64.sh -b`
    - Second set of commands (can copy-paste multiple lines)
        - `export PATH="~/miniconda3/bin:$PATH"`
        - `conda init`
- Install gcc
    - `sudo apt-get -y update`
    - `sudo apt-get -y upgrade`
    - `sudo apt-get -y install build-essential`
    - `sudo apt-get -y install libpcre3-dev`
- Install NVIDIA drivers
    - `sudo apt install ubuntu-drivers-common -y`
    - `sudo ubuntu-drivers autoinstall`
    - `sudo reboot`
    - SSH in again and confirm the installation was successful by running `nvidia-smi`
- `cd` to ARES folder and follow the rest of the README
