{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the Automated Evaluation Framework for Retrieval-Augmented Generation Systems (ARES). ARES is a groundbreaking framework for evaluating Retrieval-Augmented Generation (RAG) models. The automated process combines synthetic data generation with fine-tuned classifiers to efficiently assess context relevance, answer faithfulness, and answer relevance, minimizing the need for extensive human annotations. ARES employs synthetic query generation and Precision-Performance Iteration (PPI), providing accurate evaluations with statistical confidence. </p> <p> </p>"},{"location":"#get-started","title":"Get Started","text":"\u2699\ufe0f Installation <p>Set up ARES efficiently with our step-by-step installation guide.</p> \ud83d\udcaa  Synthetic Generation <p>Discover how to automatically create synthetic datasets that closely mimic real-world scenarios for robust RAG testing.</p> \ud83d\udcca Training Classifier <p>Learn how to train high-precision classifiers to determine the relevance and faithfulness of RAG outputs</p> \u2699\ufe0f RAG Evaluation <p>Configure RAG model evaluation with ARES to accurately evaluate your model's performance.</p>"},{"location":"installation/","title":"Installation","text":"<p>To install the necessary dependencies, run the following commands: \u200b</p> <pre><code>pip install ares-ai\n</code></pre> <p>If you would like to directly install from Github repo, run the following commands: </p> <pre><code>git clone https://github.com/stanford-futuredata/ARES.git\ncd ARES\npip install -e .\n</code></pre> <p>Optional: Initalize OpenAI or TogetherAI API key with the following commands:</p> <pre><code>export OPENAI_API_KEY=&lt;your key here&gt;\n</code></pre> <pre><code>export TOGETHER_API_KEY=&lt;your key here&gt;\n</code></pre>"},{"location":"rag_eval/","title":"RAG Evaluation","text":"This page teaches you how to configure the RAG model evaluation with ARES to accurately evaluate your model's performance."},{"location":"rag_eval/#configure-openai-api-key","title":"Configure OpenAI API Key.","text":"<pre><code>export OPENAI_API_KEY=&lt;your key here&gt;\n</code></pre>"},{"location":"rag_eval/#rag-evaluation-configuration","title":"RAG Evaluation Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the synthetic generation configuration style.</p> <pre><code>from ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": [&lt;eval_dataset_filepath&gt;],\n    \"few_shot_examples_filepath\": &lt;few_shot_filepath&gt;,\n    \"checkpoints\": [&lt;checkpoint_filepath&gt;],\n    \"labels\": [&lt;labels&gt;], \n    \"model_choice\": &lt;model_choice&gt;, # Default model is \"microsoft/deberta-v3-large\"\n    \"GPT_scoring\": &lt;True or False&gt;, \n    \"gold_label_path\": &lt;gold_label_filepath&gt;, \n    \"swap_human_labels_for_gpt4_labels\": False\n}\n\nares_module = ARES(ppi=ppi_config)\nresults = ares_module.evaluate_RAG()\nprint(results)\n</code></pre>"},{"location":"rag_eval/#evaluation-datasets","title":"Evaluation Dataset(s)","text":"<p>Input file paths to datasets for PPI evaluation, which should contain labeled data for validating classifier performance.</p> <pre><code>    \"evaluation_datasets\": [\"/data/datasets_v2/nq/nq_ratio_0.5_.tsv\"],\n</code></pre> <p>Link to ARES Github Repo for evaluation dataset example file used. </p>"},{"location":"rag_eval/#few-shot-prompt-file-path","title":"Few-Shot Prompt File Path","text":"<p>Specify the file path for a file with few-shot examples, which PPI uses to understand the labeling schema and guide the evaluation.</p> <pre><code>    \"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n</code></pre> <p>Link to ARES Github Repo for few-shot file example used. </p>"},{"location":"rag_eval/#checkpoints-file-path","title":"Checkpoint(s) File Path","text":"<p>Generated from ARES Training Classifier, provide file path(s) to model checkpoint file(s), representing the saved states of the trained classifiers used for evaluation.</p> <pre><code>\"checkpoints\": [\"output/checkpoint_generated_from_training_classifier\"],\n</code></pre>"},{"location":"rag_eval/#labels","title":"Labels","text":"<p>List the names of label columns or individua label column in your dataset(s) that PPI will use for evaluation metrics.</p> <pre><code>    \"labels\": [\"Context_Relevance_Label\"], \n</code></pre>"},{"location":"rag_eval/#gpt_scoring","title":"GPT_scoring","text":"<p>Set this flag to True if you want to use a GPT model for scoring; False uses the trained classifiers' scores.</p> <pre><code>    \"GPT_scoring\": False,\n</code></pre>"},{"location":"rag_eval/#gold-label-path","title":"Gold Label Path","text":"<pre><code>    \"gold_label_path\": \"/data/datasets_v2/nq/nq_ratio_0.6_.tsv\"\n</code></pre> <p>Link to ARES Github Repo for gold label path file example used. </p>"},{"location":"rag_eval/#swapping-human-labels-for-gpt4-labels","title":"Swapping Human Labels for GPT4 Labels","text":"<p>If True, PPI replaces human-provided labels with GPT-4's labels during evaluation; if False, it uses the original human labels.</p> <pre><code>    \"swap_human_labels_for_gpt4_labels\": False\n</code></pre>"},{"location":"rag_eval/#rag-evaluation-configuration-full-example","title":"RAG Evaluation Configuration: Full Example","text":"<pre><code>from ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": ['../data/datasets_v2/nq/nq_ratio_0.6_.tsv'], \n    \"few_shot_examples_filepath\": \"../data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n    \"checkpoints\": [\"../data/checkpoints/microsoft-deberta-v3-large/output-synthetic_queries_1.tsv/5e-06_1_True_Context_Relevance_Label_ratio_0.6_reformatted_full_articles_False_validation_with_negatives_428380.pt\", \"../data/checkpoints/microsoft-deberta-v3-large/output-synthetic_queries_1.tsv/5e-06_1_True_Answer_Relevance_Label_ratio_0.6_reformatted_full_articles_False_validation_with_negatives_428380.pt\"],\n    \"labels\": [\"Context_Relevance_Label\", \"Answer_Relevance_Label\"], \n    \"GPT_scoring\": False, \n    \"gold_label_path\": \"../data/datasets_v2/nq/nq_ratio_0.6_.tsv\", \n    \"swap_human_labels_for_gpt4_labels\": False\n}\n\nares_module = ARES(ppi=ppi_config)\nresults = ares_module.evaluate_RAG()\nprint(results)\n\n</code></pre>"},{"location":"synth_gen/","title":"Synthetic Generation","text":"This page shows you how to automatically create synthetic datasets that closely mimic real-world scenarios for robust RAG testing."},{"location":"synth_gen/#configure-openai-api-key","title":"Configure OpenAI API Key.","text":"<pre><code>export OPENAI_API_KEY=&lt;your key here&gt;\n</code></pre>"},{"location":"synth_gen/#synth-gen-configuration","title":"Synth Gen Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the synthetic generation configuration style.</p> <pre><code>from ares import ARES\n\nsynth_config = { \n    \"document_filepaths\": [&lt;document_filepaths&gt;],\n    \"few_shot_prompt_filename\": few_shot_filepath,\n    \"synthetic_queries_filenames\": [&lt;synthetic_queries_filepaths&gt;],\n    \"model_choice\": &lt;model_choice&gt;,\n    \"documents_sampled\": 10000\n}\n\nares = ARES(synthetic_query_generator=synth_config)\nresults = ares.generate_synthetic_data()\nprint(results)\n</code></pre>"},{"location":"synth_gen/#document-file-paths","title":"Document File Path(s)","text":"<p>A single or list of file paths to the document(s) you want to use for generating synthetic queries. If given a list of file paths, each file path should point to a file containing raw text from which ARES can derive context for the synthetic queries. </p> <pre><code>\"document_filepaths\": [\"/data/datasets_v2/nq/nq_ratio_0.5_.tsv\"], \n</code></pre> <p>Link to ARES Github Repo for document example file used. </p>"},{"location":"synth_gen/#few-shot-prompt-file-path","title":"Few-Shot Prompt File Path","text":"<p>This refers to the file paths for a few-shot prompt file that provide examples of queries and answers for ARES to learn from. Few-shot learning uses a small amount of labeled training data to guide the generation of synthetic queries.</p> <pre><code>\"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n</code></pre> <p>Link to ARES Github Repo for few-shot file example used. </p>"},{"location":"synth_gen/#synthetic-queries-filepath","title":"Synthetic Queries Filepath","text":"<p>A list of file paths where the generated synthetic queries will be saved. These files will store the queries created by ARES for use in training or evaluation. </p> <p>NOTE - List Size Verification</p> <p>Ensure the synthetic queries file paths list matches the document file paths list in size for consistency.</p> <pre><code>\"synthetic_queries_filenames\": [\"/output/synthetic_queries_1.tsv\"],\n</code></pre>"},{"location":"synth_gen/#model-choice","title":"Model Choice","text":"<p>Specifies the pre-trained language model to fine-tune for classification. By default, ARES uses \"microsoft/deberta-v3-large\". You can replace this with any Hugging Face model suitable for your task.</p> <pre><code> \"model_choice\": \"google/flan-t5-xxl\",\n</code></pre>"},{"location":"synth_gen/#documents-sampled","title":"Documents Sampled","text":"<p>An integer indicating how many documents to sample from your dataset when generating synthetic queries. Sampling can help speed up processing and manage computational resources. Choose a value that represents a large enough sample to generate meaningful synthetic queries, but not so large as to make processing infeasible. ARES will automatically filter documents</p> <p>NOTE - Document Filter</p> <p>ARES will automatically filter documents less than 50 words</p> <pre><code>\"documents_sampled\": 10000,\n</code></pre>"},{"location":"synth_gen/#synthetic-generation-configuration-full-example","title":"Synthetic Generation Configuration: Full Example","text":"<pre><code>from ares import ARES\n\nsynth_config = { \n    \"document_filepaths\": [\"/data/datasets_v2/nq/nq_ratio_0.5_.tsv\"],\n    \"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n    \"synthetic_queries_filenames\": [\"/output/synthetic_queries_1.tsv\"],\n    \"model_choice\": \"google/flan-t5-xxl\",\n    \"documents_sampled\": 10000\n}\n\nares = ARES(synthetic_query_generator=synth_config)\nresults = ares.generate_synthetic_data()\nprint(results)\n</code></pre>"},{"location":"training_classifier/","title":"Training Classifier","text":"This pages teach you how to train high-precision classifiers to determine the relevance and faithfulness of RAG outputs"},{"location":"training_classifier/#configure-openai-api-key","title":"Configure OpenAI API Key.","text":"<pre><code>export OPENAI_API_KEY=&lt;your key here&gt;\n</code></pre>"},{"location":"training_classifier/#training-classifier-configuration","title":"Training Classifier Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the training classifier configuration style.</p> <pre><code>from ares import ARES\n\nclassifier_config = {\n    \"classification_dataset\": [&lt;classification_dataset_filepath&gt;],\n    \"test_set_selection\": &lt;test_set_selection_filepath&gt;, \n    \"label_column\": [&lt;labels&gt;], \n    \"model_choice\": \"microsoft/deberta-v3-large\", # Default model is \"microsoft/deberta-v3-large\"\n    \"num_epochs\": 10, \n    \"patience_value\": 3, \n    \"learning_rate\": 5e-6\n}\n\nares = ARES(classifier_model=classifier_config)\nresults = ares.train_classifier()\nprint(results)\n\n</code></pre>"},{"location":"training_classifier/#classification-dataset","title":"Classification Dataset","text":"<p>Generated from the ARES synthetic generator, here you should provide a list of file paths or an individual filepath to your labeled dataset used for training the classifier. The dataset should include text data and corresponding labels for supervised learning.</p> <pre><code>\"classification_dataset\": [\"output/synthetic_queries_1.tsv\"],\n</code></pre>"},{"location":"training_classifier/#test-set-selection","title":"Test Set Selection","text":"<p>Provide the file path to your test set for evaluating the classifier's performance. This should be separate from the training data to ensure an unbiased assessment.</p> <pre><code>\"test_set_selection\": \"/data/datasets_v2/nq/nq_ratio_0.6_.tsv\"\n</code></pre> <p>Link to ARES Github Repo for test set selection file example used. </p>"},{"location":"training_classifier/#label-columns","title":"Label Column(s)","text":"<p>List the column name(s) in your dataset that contain the label(s). These are the targets your classifier will predict.</p> <pre><code>\"label_column\": [\"Conmtext_Relevance_Label\"], \n</code></pre>"},{"location":"training_classifier/#model-choice","title":"Model Choice","text":"<p>Specifies the pre-trained language model to fine-tune for classification. By default, ARES uses \"microsoft/deberta-v3-large\". You can replace this with any Hugging Face model suitable for your task.</p> <pre><code> \"model_choice\": \"google/flan-t5-xxl\",\n</code></pre>"},{"location":"training_classifier/#num-epochs","title":"Num Epochs","text":"<p>Determines the number of training epochs, which is the number of times the learning algorithm will work through the entire training dataset.</p> <pre><code>\"num_epochs\": 10, \n</code></pre>"},{"location":"training_classifier/#patience-value","title":"Patience Value","text":"<p>This is used in early stopping to prevent overfitting. It's the number of epochs with no improvement on the validation set after which training will be stopped.</p> <pre><code>\"patience_value\": 3, \n</code></pre>"},{"location":"training_classifier/#learning-rate","title":"Learning Rate","text":"<p>Sets the initial learning rate for the optimizer. This is a crucial hyperparameter that controls the adjustment of model weights during training. </p> <pre><code> \"learning_rate\": 5e-6\n</code></pre>"},{"location":"training_classifier/#training-classifier-configuration-full-example","title":"Training Classifier Configuration: Full Example","text":"<pre><code>from ares import ARES\n\nclassifier_config = {\n    \"classification_dataset\": [\"output/synthetic_queries_1.tsv\"], \n    \"test_set_selection\": \"./datasets_v2/nq/ratio_0.5_reformatted_full_articles_False_validation_with_negatives.tsv\",\n    \"label_column\": [\"Context_Relevance_Label\"], \n    \"model_choice\": \"microsoft/deberta-v3-large\",\n    \"num_epochs\": 10, \n    \"patience_value\": 3, \n    \"learning_rate\": 5e-6\n}\n\nares = ARES(classifier_model=classifier_config)\nresults = ares.train_classifier()\nprint(results)\n</code></pre>"}]}